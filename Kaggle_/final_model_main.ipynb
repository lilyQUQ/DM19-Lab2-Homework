{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "from matplotlib import pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score\n",
    "#from scipy.interpolate import spline\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotion = pd.read_csv('./trainData/emotion.csv')\n",
    "emotion = Emotion['emotion']\n",
    "seaborn.barplot(x=emotion.value_counts().index, \n",
    "                y=emotion.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsize = 240000\n",
    "emotion_list = emotion.unique()\n",
    "idx_train=[]\n",
    "for emo in emotion_list:\n",
    "    size = 0\n",
    "    for i,e in enumerate(labs['emotion']):\n",
    "        if e == emo and size <maxsize:\n",
    "            size+=1\n",
    "            idx_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = joblib.load('./fea_sel/label/label8_ver2.pkl')\n",
    "#labs = joblib.load('./fea_sel/label/label_final.pkl')\n",
    "X_train = joblib.load('./fea_sel/embed_model/emb_train.pkl')[idx_train]\n",
    "Y_train = pd.Series(y_data)[idx_train]\n",
    "X_test = joblib.load('./fea_sel/embed_model/emb_test.pkl')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.barplot(x=Y_train.value_counts().index, \n",
    "                y=Y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y  = lb.fit_transform(Y_train)\n",
    "\n",
    "sample_weight = compute_sample_weight('balanced', Y_train)\n",
    "np.unique(sample_weight, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Input, Dense, Embedding, \n",
    "                          Conv2D, SeparableConv2D, \n",
    "                          MaxPool2D, AvgPool2D, \n",
    "                          Add, Multiply, Subtract)\n",
    "from keras.layers import (Reshape, Flatten, Dropout, \n",
    "                          Concatenate, BatchNormalization, \n",
    "                          LeakyReLU, Activation, MaxoutDense)\n",
    "from keras.constraints import max_norm\n",
    "from keras.activations import softplus, tanh, relu, elu\n",
    "from keras import regularizers\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = joblib.load('./fea_sel/embed_model/tweet_model_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(y_val, y_val_pred, sample_weight):\n",
    "    \n",
    "    print(classification_report(y_true=np.argmax(y_val, axis=1), \n",
    "                                y_pred=np.argmax(y_val_pred, axis=1),\n",
    "                                target_names=list(lb.classes_))\n",
    "                                )\n",
    "    print(classification_report(y_true=np.argmax(y_val, axis=1), \n",
    "                                y_pred=np.argmax(y_val_pred, axis=1),\n",
    "                                target_names=list(lb.classes_),\n",
    "                                sample_weight=sample_weight))\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(inputs, embedding, max_document_length, trainable=False, max_norm_value=25):\n",
    "    input_dim, output_dim = embedding.shape\n",
    "    embedding_layer = Embedding(input_dim=input_dim,\n",
    "                                output_dim=output_dim,\n",
    "                                weights=[embedding],\n",
    "                                input_length=max_document_length,\n",
    "                                trainable=trainable,\n",
    "                                embeddings_constraint=max_norm(max_norm_value))(inputs)\n",
    "    embedding_layer = Reshape((max_document_length, embedding.shape[1], 1))(embedding_layer)\n",
    "    return embedding_layer\n",
    "def conv2D(inputs, filter_sizes, num_filters, embedding_dim, max_document_length, activation=None):\n",
    "    conv2D_list = []\n",
    "    for size in filter_sizes:\n",
    "        conv = Conv2D(num_filters, \n",
    "                    kernel_size=(size, embedding_dim), \n",
    "                    padding='valid',\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    kernel_constraint=max_norm(200),\n",
    "                    bias_initializer='zeros')(inputs)\n",
    "        conv = LeakyReLU()(conv)\n",
    "        pool_size = (max_document_length - size + 1, 1)\n",
    "        pooled = MaxPool2D(pool_size=pool_size, \n",
    "                            strides=(1, 1), \n",
    "                            padding='valid')(conv)\n",
    "        conv2D_list.append(pooled)\n",
    "        \n",
    "    concatenated_tensor = Concatenate(axis=1)(conv2D_list)\n",
    "    concatenated_tensor = Flatten()(concatenated_tensor)\n",
    "    return concatenated_tensor\n",
    "def scheduler(epoch):\n",
    "\n",
    "    if epoch % 20 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr changed to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "def model(filter_sizes=(3, 3, 3), \n",
    "                   num_filters=150, \n",
    "                   embedd_trainable=False, \n",
    "                   activation=None):\n",
    "    K.clear_session()\n",
    "    inputs_2 = Input(shape=(max_document_length, ), dtype='int32')\n",
    "    embedding_2 = Embedding(inputs_2, \n",
    "                                   embedding, \n",
    "                                   max_document_length, \n",
    "                                   trainable=False)\n",
    "    embedding_3 = Embedding(inputs_2, \n",
    "                                   embedding, \n",
    "                                   max_document_length, \n",
    "                                   trainable=True)\n",
    "    embedding_2 = Add()([embedding_2, embedding_3])\n",
    "    concatenated_tensor_2 = conv2D(embedding_2, \n",
    "                                        filter_sizes, \n",
    "                                        num_filters, \n",
    "                                        100, \n",
    "                                        max_document_length)\n",
    "    concatenated_tensor = Dropout(0.35)(concatenated_tensor_2)\n",
    "   \n",
    "    dense_0 = Dense(units=400, \n",
    "                    activation='tanh',\n",
    "                    kernel_constraint=max_norm(30))(concatenated_tensor)\n",
    "    dense_1 = MaxoutDense(output_dim=32,\n",
    "                           nb_feature=4,\n",
    "                           W_constraint=max_norm(3))(dense_0)\n",
    "    output  = Dense(units=num_label, \n",
    "                    activation='softmax',\n",
    "                    kernel_constraint=max_norm(3))(dense_1)\n",
    "    \n",
    "    model = Model(inputs=[inputs_2], outputs=output)\n",
    "    model.compile(optimizer=SGD(lr=0.1, decay=1e-6, momentum=0.9), \n",
    "                  loss=categorical_crossentropy, \n",
    "                  metrics=[f1])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "control panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_label = len(lb.classes_)\n",
    "max_document_length = X_train.shape[1]\n",
    "batch_size = 128\n",
    "embedding_dim = embedding.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.zeros(X_train.shape[0])\n",
    "for i, (train_index, val_index) in enumerate(skf.split(fake, Y_train)):\n",
    "    x_train = X_train[train_index]\n",
    "    y_train = y[train_index]\n",
    "    x_val = X_train[val_index]\n",
    "    y_val = y[val_index]\n",
    "    (sample_weight_train, \n",
    "     sample_weight_val) = (sample_weight[train_index], sample_weight[val_index])\n",
    "    path = './fea_sel/embed_model/{}_weights.best.hdf5'.format(i)\n",
    "    checkpoint = ModelCheckpoint(path, \n",
    "                                 monitor='val_f1', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='max')\n",
    "    print(\"Creating Model...\")\n",
    "    model = model()\n",
    "    model_hist = model.fit([x_train], y_train,\n",
    "                           sample_weight=None,\n",
    "                           validation_data=([x_val], y_val, None),\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True,\n",
    "                           epochs=40, \n",
    "                           verbose=1,\n",
    "                           callbacks=[checkpoint,reduce_lr])\n",
    "    model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prediction with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def writer_csv(logPath, logging):\n",
    "    f = open(logPath,'a')\n",
    "    w = csv.writer(f,lineterminator = '\\r')\n",
    "    w.writerow(logging)\n",
    "    f.close() \n",
    "\n",
    "model.load_weights(path)              # loading the weights of best model\n",
    "y_test_pred = model.predict([X_test])  # predict with the test dataset    \n",
    "test_pred=lb.inverse_transform(y_test_pred) #inverse_transform the prediction into number(0-7)\n",
    "testID = joblib.load('./fea_sel/test/ID.pkl')\n",
    "\n",
    "logg = './submission12.csv'   \n",
    "writer_csv(logg, ['id','emotion'])\n",
    "\n",
    "# replace the numeric label with emotions\n",
    "for id_,emo in zip(testID,test_pred):\n",
    "    if emo == 0: w = 'sadness'\n",
    "    elif emo == 1: w = 'disgust'\n",
    "    elif emo == 2: w = 'anticipation'\n",
    "    elif emo == 3: w = 'joy'\n",
    "    elif emo == 4: w = 'trust'\n",
    "    elif emo == 5: w = 'anger'\n",
    "    elif emo == 6: w = 'fear'\n",
    "    elif emo == 7: w = 'surprise'\n",
    "    ww= [id_,w]\n",
    "    writer_csv(logg, ww)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
