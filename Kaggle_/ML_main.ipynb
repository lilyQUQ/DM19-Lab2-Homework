{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_tool import loadData\n",
    "import data_tool\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 999\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "#path\n",
    "feaDir = './fea_sel/fea_elmo.pkl'\n",
    "labDir = './fea_sel/lab6.pkl'\n",
    "modPath = './model/'\n",
    "#params\n",
    "gbm_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':8,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 30,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 6,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_iterations': 2000,\n",
    "    'verbose': 1,\n",
    "    'device': 'cpu',\n",
    "    'n_jobs': 6,\n",
    "#    'gpu_platform_id': 0,\n",
    "#    'gpu_device_id': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML(fea_train,fea_test, lab_train, lab_test,method):\n",
    "    \n",
    "    if method == 'svm': \n",
    "        clf = SVC(gamma='scale',C = 5)\n",
    "        clf.fit(fea_train,lab_train)    \n",
    "    elif method == 'LR':\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(fea_train,lab_train)    \n",
    "    elif method == 'NB':\n",
    "        clf = MultinomialNB()\n",
    "        clf.fit(fea_train,lab_train)\n",
    "    elif method == 'RF':\n",
    "        clf=RandomForestClassifier(criterion='entropy') \n",
    "        clf.fit(fea_train,lab_train)\n",
    "    elif method == 'gdbt':\n",
    "        clf=None\n",
    "        lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        clf = lgb.train(gbm_params, lgb_train, valid_sets=lgb_train, init_model=clf)    \n",
    "    \n",
    "    pred = clf.predict(fea_test)\n",
    "    if method =='gdbt':\n",
    "        pred = np.argmax(pred,axis=1)\n",
    "    gnd = lab_test\n",
    "    return pred,gnd   \n",
    "\n",
    "def writer_csv(logPath, logging):\n",
    "    f = open(logPath,'a')\n",
    "    w = csv.writer(f,lineterminator = '\\r')\n",
    "    w.writerow(logging)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#     0. Loading feature and split\n",
    "# =============================================================================\n",
    "# train = joblib.load('./fea_sel/text/fea-2_tr.pkl')\n",
    "tfidf_tr = joblib.load('./fea_sel/tfidf/tfidf_train-832.pkl')\n",
    "bow_tr = joblib.load('./fea_sel/BOW/bow_train-832.pkl')\n",
    "label8 = joblib.load('./fea_sel/label/label8_ver2.pkl')\n",
    "#    label8=label8[0].apply(lambda x:int(x))\n",
    "#    leng = 1000\n",
    "\n",
    "FEATURE = {'tfidf':tfidf_tr,'bow':bow_tr}\n",
    "LABEL = {'lab8':label8}\n",
    "best_result ={'f1':0}\n",
    "for fea_name, feature in FEATURE.items():\n",
    "    for lab_name, label in LABEL.items():\n",
    "        leng=100000\n",
    "#            feature = feature[:leng]\n",
    "#            label = label[:leng]\n",
    "        x_train,x_valid,y_train,y_valid = train_test_split(feature,label, random_state =seed,test_size = 0.4 )\n",
    "        clf = SGDClassifier(loss='squared_loss',max_iter=1000,tol=1e-3,n_jobs=6, \\\n",
    "                            early_stopping=True,learning_rate='adaptive',eta0=0.001,penalty='elasticnet',class_weight=\"balanced\")\n",
    "        # for pc in range(5, 25, 5):\n",
    "\n",
    "            # clf = SVC(gamma='scale',C = 5)\n",
    "            # pipe_clf = make_pipeline(SelectPercentile(f_classif, percentile=pc/100),clf)\n",
    "            # pipe_clf.fit(x_train, y_train)\n",
    "            # print('Now Selection Percentile is : ', pc)\n",
    "            # score = pipe_clf.score(x_valid, y_valid)\n",
    "            # pred = pipe_clf.predict(x_valid)\n",
    "            # gnd = y_valid\n",
    "            # cm = confusion_matrix(gnd, pred)\n",
    "#            clf = SVC(gamma='scale',C = 5)\n",
    "        clf.fit(x_train, y_train)\n",
    "        pred = clf.predict(x_valid)\n",
    "        gnd = y_valid\n",
    "        cm = confusion_matrix(gnd, pred)\n",
    "    # =============================================================================\n",
    "    #     1. ML\n",
    "    # =============================================================================\n",
    "        # METHOD = ['svm','LR','NB','RF', 'gdbt']\n",
    "        # for i,method in enumerate(METHOD):\n",
    "        #     if i < 5:\n",
    "        #         print('======== ML: {} ======'.format(method))\n",
    "        #         pred,gnd = ML(x_train,x_valid,y_train,y_valid,method = method)\n",
    "        #     else:\n",
    "        #         print('======== DL: {} ======',format(method))\n",
    "        #         # ==== 2. DNN ====\n",
    "\n",
    "        #         # ==== 3. LSTM ====\n",
    "\n",
    "        f1_list = data_tool.eachAccu(pred,gnd)\n",
    "        f1_overall = f1_score(pred,gnd,average='micro')\n",
    "        print('\\nf1_overall:{:.4}'.format(f1_overall))\n",
    "        if f1_overall>best_result['f1']:\n",
    "            best_result['f1'] = f1_overall\n",
    "            # best_result['model'] = method\n",
    "            best_result['feature'] = fea_name\n",
    "            best_result['label'] = lab_name\n",
    "            # best_result['pc'] = pc\n",
    "        #======== Test Subimission ====#\n",
    "        test_data = joblib.load('./fea_sel/BOW/bow_test-832.pkl')\n",
    "        testID = joblib.load('./fea_sel/test/testID.pkl')\n",
    "        pred = clf.predict(test_data)\n",
    "        # {'0':'sadness' ,'1':'disgust' ,'2':'anticipation', '3':'joy' ,'4':'trust' ,'5':'anger', '6':'fear','7':'surprise'}\n",
    "        logg = './submission2.csv'\n",
    "        writer_csv(logg, ['id','emotion'])\n",
    "        for id_,emo in zip(testID,pred):\n",
    "            if emo == 0: w = 'sadness'\n",
    "            elif emo == 1: w = 'disgust'\n",
    "            elif emo == 2: w = 'anticipation'\n",
    "            elif emo == 3: w = 'joy'\n",
    "            elif emo == 4: w = 'trust'\n",
    "            elif emo == 5: w = 'anger'\n",
    "            elif emo == 6: w = 'fear'\n",
    "            elif emo == 7: w = 'surprise'\n",
    "            ww= [id_,w]\n",
    "            writer_csv(logg, ww)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
