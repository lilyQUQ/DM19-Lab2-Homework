{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "\tdef __init__(self, input_size, embedding_dim, hidden_dim, output_size):\n",
    "\n",
    "\t\tsuper(LSTMClassifier, self).__init__()\n",
    "\n",
    "\t\tself.embedding_dim = embedding_dim\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.input_size = input_size\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "\t\tself.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1)\n",
    "\n",
    "\t\tself.hidden2out = nn.Linear(hidden_dim, output_size)\n",
    "\t\tself.softmax = nn.LogSoftmax()\n",
    "\n",
    "\t\tself.dropout_layer = nn.Dropout(p=0.2)\n",
    "\n",
    "\n",
    "\tdef init_hidden(self, batch_size):\n",
    "\t\treturn(autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)),\n",
    "\t\t\t\t\t\tautograd.Variable(torch.randn(1, batch_size, self.hidden_dim)))\n",
    "\n",
    "\n",
    "\tdef forward(self, batch, lengths):\n",
    "\t\t\n",
    "\t\tself.hidden = self.init_hidden(batch.size(-1))\n",
    "\n",
    "\t\tembeds = self.embedding(batch)\n",
    "\t\tpacked_input = pack_padded_sequence(embeds, lengths)\n",
    "\t\toutputs, (ht, ct) = self.lstm(packed_input, self.hidden)\n",
    "\n",
    "\t\t# ht is the last hidden state of the sequences\n",
    "\t\t# ht = (1 x batch_size x hidden_dim)\n",
    "\t\t# ht[-1] = (batch_size x hidden_dim)\n",
    "\t\toutput = self.dropout_layer(ht[-1])\n",
    "\t\toutput = self.hidden2out(output)\n",
    "\t\toutput = self.softmax(output)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN2(nn.Module):\n",
    "    def __init__(self, layer_sizes, output_size, drop_p=0.5):\n",
    "        super().__init__()\n",
    "        layer_sizes = deepcopy(layer_sizes)\n",
    "        self.MLP = nn.Sequential()\n",
    "        for i, (in_size, out_size) in enumerate( zip(layer_sizes[:-1], layer_sizes[1:]) ):\n",
    "            self.MLP.add_module(name=\"L%i\"%(i), module=nn.Linear(in_size, out_size)),\n",
    "            self.MLP.add_module(name=\"B%i\"%(i), module=nn.BatchNorm1d(out_size)),\n",
    "            self.MLP.add_module(name=\"A%i\"%(i), module=nn.LeakyReLU()),\n",
    "            self.MLP.add_module(name=\"D%i\"%(i), module=nn.Dropout(p=drop_p))\n",
    "            nn.init.xavier_normal_(self.MLP[0].weight.data, gain=1.414)\n",
    "        self.out = nn.Linear(layer_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        y = self.out(x)\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
