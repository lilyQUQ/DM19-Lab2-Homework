{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Data Preprocess:\n",
    "    1. change some abbreviated words into the normal form\n",
    "    2. remove URL\n",
    "    3. remove \"<LH>\" in the sentences\n",
    "    4. lemmatize\n",
    "    5.  & tokenization\n",
    "- Feature Extraction by 1. BOW 2.TFIDF \n",
    "- Feature Extraction by selectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,Dataset\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras import backend as K\n",
    "#import keras.layers as layers\n",
    "#from keras.models import Model, load_model\n",
    "#from keras.engine import Layer\n",
    "#import tensorflow_hub as hub\n",
    "#import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from tensorflow.python.client import device_lib \n",
    "import multiprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import_ipynb\n",
      "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Building wheel for import-ipynb (setup.py): started\n",
      "  Building wheel for import-ipynb (setup.py): finished with status 'done'\n",
      "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2982 sha256=1a46ead5d354f0a8fb0488bba2765231cb7c235c0971dfefc598fb11323465e0\n",
      "  Stored in directory: C:\\Users\\Lily\\AppData\\Local\\pip\\Cache\\wheels\\b4\\7b\\e9\\a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n",
      "importing Jupyter notebook from data_tool.ipynb\n",
      "importing Jupyter notebook from clean_tool.ipynb\n"
     ]
    }
   ],
   "source": [
    "%run clean_tool.ipynb\n",
    "!pip install import_ipynb\n",
    "import import_ipynb\n",
    "import data_tool \n",
    "import clean_tool as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Raw Data...\n",
      "Loading training set...\n",
      "Loading testing set...\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "X_data, Y_data, X_val = data_tool.loadData()\n",
    "# Path or Direction  \n",
    "savPath = './fea_sel/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1021</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index           _crawldate   _type  \\\n",
       "0     391  hashtag_tweets  2015-05-23 11:42:47  tweets   \n",
       "1     433  hashtag_tweets  2016-01-28 04:52:09  tweets   \n",
       "3     376  hashtag_tweets  2016-01-24 23:53:05  tweets   \n",
       "5     120  hashtag_tweets  2015-06-11 04:44:05  tweets   \n",
       "6    1021  hashtag_tweets  2015-08-18 02:30:07  tweets   \n",
       "\n",
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "3                             []  0x1cd5b0   \n",
       "5      [authentic, LaughOutLoud]  0x1d755c   \n",
       "6                             []  0x2c91a8   \n",
       "\n",
       "                                                text  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  \n",
       "3                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  \n",
       "6       Still waiting on those supplies Liscus. <LH>  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x376b20  anticipation\n",
       "1  0x2d5350       sadness\n",
       "2  0x1cd5b0          fear\n",
       "3  0x1d755c           joy\n",
       "4  0x2c91a8  anticipation"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>104</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>310</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _score          _index           _crawldate   _type  \\\n",
       "2      232  hashtag_tweets  2017-12-25 04:39:20  tweets   \n",
       "4      989  hashtag_tweets  2016-01-08 17:18:59  tweets   \n",
       "9       66  hashtag_tweets  2015-09-09 09:22:55  tweets   \n",
       "30     104  hashtag_tweets  2015-10-10 14:33:26  tweets   \n",
       "33     310  hashtag_tweets  2016-10-23 08:49:50  tweets   \n",
       "\n",
       "                             hashtags  tweet_id  \\\n",
       "2                        [bibleverse]  0x28b412   \n",
       "4                                  []  0x2de201   \n",
       "9   [materialism, money, possessions]  0x218443   \n",
       "30               [GodsPlan, GodsWork]  0x2939d5   \n",
       "33                                 []  0x26289a   \n",
       "\n",
       "                                                 text  \n",
       "2   Confident of your obedience, I write to you, k...  \n",
       "4   \"Trust is not the same as faith. A friend is s...  \n",
       "9   When do you have enough ? When are you satisfi...  \n",
       "30  God woke you up, now chase the day #GodsPlan #...  \n",
       "33  In these tough times, who do YOU turn to as yo...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data,col='text'):\n",
    "    # data=text_tr\n",
    "    # data1=text_tr['text'][idx]\n",
    "    '''\n",
    "    remove unneccessary URL,whitespace,punctuation\n",
    "    lemmatization or stemming \n",
    "    tokenization\n",
    "    !! Not Sure should or should not keep ',' '.'\n",
    "    '''\n",
    "    data['clean_sent'] = ct.FirstClean(data[col])\n",
    "    data['clean_sent'] = ct.removeURL(data['clean_sent'])\n",
    "    data['clean_sent'] = ct.removeWP(data['clean_sent'])\n",
    "    data['clean_sent'] = ct.removeLH(data['clean_sent'])\n",
    "    data['clean_sent'] = ct.lemma(data['clean_sent'])  \n",
    "    data['clean_sent'] = ct.preprocess(data['clean_sent'])\n",
    "    data['clean_sent'] = ct.removePUNC(data['clean_sent'])\n",
    "    # data['clean_sent'] = ct.tokenizer(data['clean_sent'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    '''\n",
    "    train: a list of text \n",
    "    '''\n",
    "    def __init__(self,train,test = None):\n",
    "        self.train=train\n",
    "        self.test =test\n",
    "        self.model = None\n",
    "#    def word2vec(self):\n",
    "#        from gensim.models import word2vec\n",
    "#        from gensim import models\n",
    "        \n",
    "    def BOW(self):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        self.model = CountVectorizer(lowercase=True,max_features = 2048)\n",
    "        # bow_tr,bow_ts = self.run()\n",
    "        bow_tr = self.model.fit_transform(self.train)\n",
    "        bow_ts = self.model.transform(self.test)\n",
    "        return bow_tr,bow_ts\n",
    "    \n",
    "    def tfidf(self):\n",
    "        from sklearn.feature_extraction.text import TfidfTransformer\n",
    "        \n",
    "        X_tr, X_ts  =self.BOW()\n",
    "        self.model = TfidfTransformer()\n",
    "        # tfidf_tr,tfidf_ts = self.run()\n",
    "        tfidf_tr = self.model.fit_transform(X_tr)\n",
    "        tfidf_ts = self.model.transform(X_ts)\n",
    "        return tfidf_tr,tfidf_ts\n",
    "    \n",
    "    def run(self):\n",
    "        self.model.fit(self.train)\n",
    "        emb_tr = self.model.transform(self.train)\n",
    "        # if self.test.empty == False :\n",
    "        emb_ts = self.model.transform(self.test)\n",
    "        return emb_tr,emb_ts\n",
    "        return emb_tr,emb_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(Y_data):\n",
    "    '''\n",
    "    to transfer the emotion into 0-7 by orders\n",
    "    '''\n",
    "    emotion = ['sadness' ,'disgust' ,'anticipation', 'joy' ,'trust' ,'anger', 'fear','surprise']\n",
    "    label = pd.DataFrame(np.zeros(len(Y_data)))\n",
    "    for i,emo in enumerate(emotion):\n",
    "        label.loc[Y_data['emotion']==emo]=i\n",
    "    joblib.dump(label,'./fea_sel/label/label8.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=16)\n",
    "col = ['_score','hashtags','tweet_id','text']\n",
    "text_tr = X_data[col]\n",
    "text_ts = X_val[col]\n",
    "\n",
    "print('start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization consumes lots of time, use muktiprocessing to speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-934b5b0aec3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 1. Clean and lemmaitize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#joblib.dump(data_tr, './fea_sel/text/data_train.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#joblib.dump(data_ts, './fea_sel/test/data_test.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-5c96596b89e5>\u001b[0m in \u001b[0;36mclean_data\u001b[1;34m(data, col)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0;31m!\u001b[0m\u001b[0;31m!\u001b[0m \u001b[0mNot\u001b[0m \u001b[0mSure\u001b[0m \u001b[0mshould\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mshould\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkeep\u001b[0m \u001b[1;34m','\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     '''\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_sent'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFirstClean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_sent'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveURL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_sent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_sent'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveWP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_sent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DMlab\\DMlab2\\Kaggle_\\clean_tool.ipynb\u001b[0m in \u001b[0;36mFirstClean\u001b[1;34m(data)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\DMlab\\DMlab2\\Kaggle_\\clean_tool.ipynb\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Clean and lemmaitize  \n",
    "data_tr = pool.map(clean_data(text_tr))\n",
    "data_ts = pool.map(clean_data(text_ts))\n",
    "#joblib.dump(data_tr, './fea_sel/text/data_train.pkl')\n",
    "#joblib.dump(data_ts, './fea_sel/test/data_test.pkl')\n",
    "\n",
    "sent_tr = data_tr['clean_sent']\n",
    "sent_ts = data_ts['clean_sent']\n",
    "# sent_tr=sent_tr.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wordembedding\n",
    "# 2-1. word2vec\n",
    "# 2-2. tfidf\n",
    "emb_tr,emb_ts = Embedding(sent_tr,sent_ts).tfidf()\n",
    "\n",
    "# 2-3. BOW \n",
    "emb_tr,emb_ts  = Embedding(sent_tr,sent_ts).BOW()\n",
    "\n",
    "# 2-4. glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. other feature\n",
    "hashtag_tr = data_tr['hashtags'].apply(lambda x: ' '.join(x))\n",
    "hashtag_ts = data_ts['hashtags'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# tmp_tr = pool.map(ct.lemma(hashtag_tr))\n",
    "# tmp_ts = pool.map(ct.lemma(hashtag_ts))\n",
    "\n",
    "emb_tr,emb_ts  = Embedding(hashtag_tr,hashtag_ts).BOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. feature selection, dimension reduction \n",
    "# pca = PCA(n_components=2)\n",
    "y=joblib.load('./fea_sel/label/label8.pkl')\n",
    "sel_f = SelectKBest(f_classif, k=512)\n",
    "X_train_f = sel_f.fit_transform(emb_tr, y)\n",
    "X_test_f = sel_f.transform(emb_ts)\n",
    "joblib.dump(X_train_f,'./fea_sel/BOW/bow_train-512.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. concat. other feature and main feature\n",
    "XX = joblib.load('./fea_sel/BOW/bow_test-2048.pkl')\n",
    "xx = joblib.load('./fea_sel/hashtag_test_320.pkl')\n",
    "import scipy.sparse as sp\n",
    "con = sp.hstack((XX, xx))\n",
    "con.shape\n",
    "joblib.dump(con,'./fea_sel/BOW/bow_test-2368.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
