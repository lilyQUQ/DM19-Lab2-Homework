{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for raw json data reading and some funcitons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "from tqdm import tqdm\n",
    "\n",
    "#import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = './trainData/tweets_DM.json'\n",
    "identiPath = './trainData/data_identification.csv'\n",
    "labelPath = './trainData/emotion.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRawTweet(dataPath):\n",
    "    '''\n",
    "    Inputs:\n",
    "        dataPath:  = './trainData/tweets_DM.json'\n",
    "    Outputs:\n",
    "         twee_df: data frame aftre sortation\n",
    "    '''\n",
    "    with open(dataPath, encoding = 'utf-8-sig') as json_file:    \n",
    "    #文件不是標準json文件，故要先將文件重新轉回json\n",
    "    # the file is not standard json file, so I transform the file into json\n",
    "        data = (line.strip() for line in json_file)    # 因為文件有多行，直接讀取會出錯，因此一行一行讀取  # read by lines   \n",
    "        tmp = '[{}]'.format(','.join(data))\n",
    "        twee_df = pd.read_json(tmp)\n",
    "    \n",
    "    # 整理'_source'裡的資料: # sorting out the data in \"_source\"\n",
    "    info = {i:[] for i in twee_df['_source'][0]['tweet'].keys()}  \n",
    "    for idx, tweet in enumerate(twee_df['_source']):  \n",
    "        for key,data in tweet['tweet'].items():\n",
    "            info[key].append(data)\n",
    "            \n",
    "    new = pd.DataFrame(info)\n",
    "    twee_df = pd.concat([twee_df,new],axis=1)\n",
    "#    twee_df.drop(columns = ['_source'])\n",
    "    return twee_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    '''\n",
    "    loading raw data, and make them \"DataFame\"\n",
    "    '''\n",
    "    print('Loading Raw Data...')\n",
    "    rawData = loadRawTweet(dataPath)\n",
    "    rawData = rawData.drop(['_source'],axis = 1)\n",
    "    label = pd.read_csv(labelPath)\n",
    "    identi = pd.read_csv(identiPath)\n",
    "    # =============================================================================\n",
    "    #     spilt into trainning set and testing set\n",
    "    # =============================================================================\n",
    "    train_id = identi.loc[identi['identification']=='train']\n",
    "    test_id  = identi.loc[identi['identification']=='test']\n",
    "    X_train = pd.DataFrame()\n",
    "    Y_train = pd.DataFrame()\n",
    "    X_test = pd.DataFrame()\n",
    "    Y_test = pd.DataFrame()\n",
    "    print('Loading training set...')\n",
    "\n",
    "    tr1 = rawData.tweet_id.isin(train_id['tweet_id'])\n",
    "    tr2 = label.tweet_id.isin(train_id['tweet_id'])\n",
    "    X_train = rawData.loc[tr1==True]\n",
    "    Y_train = label.loc[tr2==True]\n",
    "    print('Loading testing set...')\n",
    "    ts1 = rawData.tweet_id.isin(test_id['tweet_id'])\n",
    "    ts2 = label.tweet_id.isin(test_id['tweet_id'])\n",
    "    X_test = rawData.loc[ts1==True]\n",
    "    Y_test = label.loc[ts2==True] \n",
    "    \n",
    "    #--- make sure the ID in X_train and Y_train are same order\n",
    "    ID = X_train['tweet_id']\n",
    "    Y_train = pd.merge(ID,Y_train,on='tweet_id')\n",
    "    \n",
    "    return X_train, Y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(le,labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le,one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eachAccu(pred,gnd,func='f1',labName = {'0':'sadness' ,'1':'disgust' ,'2':'anticipation', '3':'joy' ,'4':'trust' ,'5':'anger', '6':'fear','7':'surprise'}):\n",
    "    from sklearn.metrics import f1_score\n",
    "#    pred2 = np.argmax(np.vstack(pred), axis=1).flatten()\n",
    "    pred3 = np.array(pred)\n",
    "    gnd = np.array(gnd)\n",
    "    f1_list = {}\n",
    "    for key,name in labName.items():\n",
    "        pred4 = pred3[gnd==int(key)]\n",
    "        gnd2 = gnd[gnd==int(key)]\n",
    "        if func == 'f1':\n",
    "            f1 = f1_score(pred4,gnd2,average='micro')\n",
    "        f1_list[name] = f1\n",
    "        print('f1_{}:{:4f}'.format(name,f1))\n",
    "    return f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
